{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx33Evs0dvcFXB/T/S+tNk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreeharsha243/BigData/blob/main/Bigdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zsXkcMBeSWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cead3ec-210b-45b8-bb91-511bd0a82d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(6, 'ashwin'), (6, 'Jadeja'), (5, 'virat'), (5, 'rohit'), (5, 'rahul')]\n",
            "[(6, 'ashwin'), (6, 'Jadeja')]\n",
            "[('ashwin', 6), ('Jadeja', 6)]\n"
          ]
        }
      ],
      "source": [
        "cplayers=[\"virat\",\"rahul\",\"ashwin\",\"Jadeja\",\"rohit\"]\n",
        "l1=list(map(lambda name:(len(name),name),cplayers))\n",
        "l1.sort(reverse=True)\n",
        "print(l1)\n",
        "l2=list(filter(lambda x:(x[0]==l1[0][0]),l1))\n",
        "print(l2)\n",
        "l3=list(map(lambda w:(w[1],w[0]),l2))\n",
        "print(l3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_m7wl0lhEJW",
        "outputId": "0fab2f83-87ce-4a03-8282-f2790495e570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=b95ca68c12cb1cbf1dd23366702c925498fde15eb49fa19c6277039eae2a1808\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc=SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "LpNLh945heMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize(range(1000))\n",
        "print(rdd.take(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s8u2Z6Zivw2",
        "outputId": "7a6912ad-4d3e-4b0f-d766-8b87eeabf93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "even=rdd.filter(lambda n:n%2==0)\n",
        "even.take(15)\n",
        "#we can use for the odd numbers also\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd4O_EN4jw9v",
        "outputId": "92bf181f-89f4-4c81-e6c2-1f4cb255bf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blog=sc.textFile('')\n",
        "blog.take(5) #first five lines\n",
        "\n",
        "blog.collect() #print all the lines in the text file\n",
        "\n",
        "blog.count() #for counting"
      ],
      "metadata": {
        "id": "cNBgl4Sgk3UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNCDXHtZur1b",
        "outputId": "ac49124f-3db7-4d5d-b3a2-f4b5541b95d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=a1fba1261ed6501cde5c8828b385199b3456a75d3d5c23d561a12dcac8271286\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.session import SparkSession\n",
        "sess=SparkSession.builder.getOrCreate()\n",
        "train=sess.read.csv('',header=True)\n",
        "train.count()\n",
        "train.printSchema()"
      ],
      "metadata": {
        "id": "RFG--SVgwhJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns()\n",
        "\n",
        "#find distinct products\n",
        "train.select('Product_ID').distinct().count()\n",
        "train.select('Product_ID').distinct().show(100)"
      ],
      "metadata": {
        "id": "PvRRBxn9xbzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}