{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreeharsha243/BigData/blob/main/harsha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNTRIn1xW2eH",
        "outputId": "0a80d933-6da4-4348-fb93-b238732b4350"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=d489965842701445020b99f840f51dd7caa72f4350e0b7215953c412e8cb509a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadTextFile\").getOrCreate()\n",
        "text_file_path = r'/content/sample_data/lvl.txt'\n",
        "text_data = spark.read.text(text_file_path)\n",
        "\n",
        "\n",
        "text_data.show(truncate=False)\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcRB3XI7Xc2W",
        "outputId": "b7dde9fd-dfed-4384-e2e5-d571cb3c5f5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|It only grew stronger by the day. But as always I didn’t listen to the world, the people around me. If only we could have been brave enough and stuck together. So this is what it means to be without you all these years…                                                                                                                                                                            |\n",
            "|I love you so much that I would even go as far as lip glossing one of your bras! And someday, if you’re lucky, maybe I’ll even wear it.                                                                                                                                                                                                                                                                |\n",
            "|We’ve only been together four months and I already know you are the one. You bring so much happiness into my life, I can’t imagine being without you! I love you!                                                                                                                                                                                                                                      |\n",
            "|You are my everything. I love you more than anything in the world and am so lucky to have you by my side. You mean the world to me and I hope to spend every day making you happy!                                                                                                                                                                                                                     |\n",
            "|                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|Every time I look at you, my heart skips a beat. When I feel your hugs, my strength becomes tenfold. I cannot imagine a day without you — and am certain I don’t want to know what that feels like… I love you more than words can even say; you are the light of my life, my soulmate, and my best friend.                                                                                            |\n",
            "|You look so fine, your skin so soft. I love seeing you every day. I get butterflies in my stomach anytime we’re this close. I’ve liked you for a while — you’re the one I want to be with forever and ever.                                                                                                                                                                                            |\n",
            "|So I guess what I’m trying to say is that I love you. So much. You mean the world to me and I wouldn’t trade you for anything. You are my heart, my soul, and in a million years there will never be anyone else for me but you.                                                                                                                                                                       |\n",
            "|Love is a word that shouldn’t be introduced when I am around you because it’s just not needed. It is so apparent that we are meant to be, it’s almost obnoxiously obvious.                                                                                                                                                                                                                             |\n",
            "|I hope time and erase everything that you’re feeling bad about. I love to be around you because you are so beautiful! trust me, I am telling the truth.                                                                                                                                                                                                                                                |\n",
            "|I love you. I’m not saying it because you want me to, or because I have to, or because it’s Valentines day, but because it’s true.                                                                                                                                                                                                                                                                     |\n",
            "|                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|Let’s make our relationship status official in case there are any holdouts. We’re dating an                                                                                                                                                                                                                                                                                                            |\n",
            "|I think it’s going pretty well. I don’t know of any couples that don’t run into their share of issues along the way, but as far as I can tell, we do ok.                                                                                                                                                                                                                                               |\n",
            "|You are the love of my life; you are my world. I would do anything to make you happy. I want to see your beautiful smile every moment of every day. You bring me joy and happiness, and I promise to give you all of mine. You mean the world to me, and I will forever love you.                                                                                                                      |\n",
            "|I love you so much and I am so glad I met you. You are the one I have been waiting for all of my life. The little things, like making sure to let me know when I look beautiful and holding my hand whenever you can, makes me feel so loved. You are the love of my life. Without you I am incomplete. I love you more than any woman I have ever met or ever will meet in the future. You are amazing|\n",
            "|I love you and have never been so sure of anything in my life. I could get lost in your eyes forever. Whenever I look into them they make me melt, I smile and laugh every time you kiss my neck. Your lips fit with mine perfectly like two pieces of a puzzle coming together.                                                                                                                       |\n",
            "|Every single day, I fall even more in love with you. Your smile melts my heart and your eyes send so many different emotions through me. I can’t imagine life without you.                                                                                                                                                                                                                             |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing blank lines, stop words and  punctuation characters before further processing\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "text_file_path = r'/content/sample_data/lvl.txt'\n",
        "with open(text_file_path, 'r') as file:\n",
        "    text_content = file.read()\n",
        "\n",
        "\n",
        "text_content = '\\n'.join(line for line in text_content.splitlines() if line.strip())\n",
        "\n",
        "\n",
        "tokens = word_tokenize(text_content)\n",
        "filtered_tokens = [token for token in tokens if re.match(r'^\\w+$', token)]\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = ' '.join(word for word in filtered_tokens if word.lower() not in stop_words)\n",
        "\n",
        "print(filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGZNcQeOYwGe",
        "outputId": "80b02859-b68b-48a8-8caf-d2bf2c0f078a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grew stronger day always listen world people around could brave enough stuck together means without love much would even go far lip glossing one bras someday lucky maybe even wear together four months already know one bring much happiness life imagine without love everything love anything world lucky side mean world hope spend every day making happy Every time look heart skips beat feel hugs strength becomes tenfold imagine day without certain want know feels love words even say light life soulmate best friend look fine skin soft love seeing every day get butterflies stomach anytime close liked one want forever ever guess trying say love much mean world trade anything heart soul million years never anyone else Love word introduced around needed apparent meant almost obnoxiously obvious hope time erase everything feeling bad love around beautiful trust telling truth love saying want Valentines day true Let make relationship status official case holdouts dating think going pretty well know couples run share issues along way far tell love life world would anything make happy want see beautiful smile every moment every day bring joy happiness promise give mine mean world forever love love much glad met one waiting life little things like making sure let know look beautiful holding hand whenever makes feel loved love life Without incomplete love woman ever met ever meet future amazing love never sure anything life could get lost eyes forever Whenever look make melt smile laugh every time kiss neck lips fit mine perfectly like two pieces puzzle coming together Every single day fall even love smile melts heart eyes send many different emotions imagine life without like share life like met boring old meaningless spectacular new meaningful LOVE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word with highest number of occurrence\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"RDDWordCount\")\n",
        "sc = SparkContext(conf=conf)\n",
        "text_file_path = \"/content/sample_data/lvl.txt\"\n",
        "text_rdd = sc.textFile(text_file_path)\n",
        "word_counts = text_rdd.flatMap(lambda line: line.split()).countByValue()\n",
        "most_common_word = max(word_counts, key=word_counts.get)\n",
        "print(\"Word with highest occurrence:\", most_common_word)\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW5THORrY8l9",
        "outputId": "ff03fc15-0ce6-4eb6-9f49-4d2ca30b5c83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word with highest occurrence: I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Character with highest number of occurrence\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"RDDCharacterCount\")\n",
        "sc = SparkContext(conf=conf)\n",
        "text_file_path = \"/content/sample_data/lvl.txt\"\n",
        "text_rdd = sc.textFile(text_file_path)\n",
        "character_counts = text_rdd.flatMap(lambda line: list(line)).countByValue()\n",
        "most_common_character = max(character_counts, key=character_counts.get)\n",
        "print(\"Character with highest occurrence:\", most_common_character)\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfppIAZmZCfU",
        "outputId": "00b2d97a-c6f7-4bcc-eac7-ad3beb3b8f26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character with highest occurrence:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute occurrence of any given word accepted from keyboard\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"RDDWordOccurrence\")\n",
        "sc = SparkContext(conf=conf)\n",
        "text_file_path = \"/content/sample_data/lvl.txt\"\n",
        "text_rdd = sc.textFile(text_file_path)\n",
        "target_word = input(\"Enter a word: \")\n",
        "word_occurrences = text_rdd.filter(lambda line: target_word in line).count()\n",
        "print(\"Occurrences of\", target_word, \":\", word_occurrences)\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "660WjKgvZSm0",
        "outputId": "fb931c81-27e3-4bd1-f62e-17374d1a316c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: hello\n",
            "Occurrences of hello : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Line sorted in descending order of number of words they contain\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"RDDLineSorting\")\n",
        "sc = SparkContext.getOrCreate()\n",
        "text_file_path = \"/content/sample_data/lvl.txt\"\n",
        "text_rdd = sc.textFile(text_file_path)\n",
        "lines_sorted = text_rdd.map(lambda line: (line, len(line.split()))).sortBy(lambda x: x[1], ascending=False)\n",
        "sorted_lines = lines_sorted.map(lambda x: x[0]).collect()\n",
        "for line in sorted_lines:\n",
        "    print(line)\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKjV6LGXZbIK",
        "outputId": "4476634e-6dbd-41a2-e416-27ceae12b070"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love you so much and I am so glad I met you. You are the one I have been waiting for all of my life. The little things, like making sure to let me know when I look beautiful and holding my hand whenever you can, makes me feel so loved. You are the love of my life. Without you I am incomplete. I love you more than any woman I have ever met or ever will meet in the future. You are amazing\n",
            "Every time I look at you, my heart skips a beat. When I feel your hugs, my strength becomes tenfold. I cannot imagine a day without you — and am certain I don’t want to know what that feels like… I love you more than words can even say; you are the light of my life, my soulmate, and my best friend.\n",
            "You are the love of my life; you are my world. I would do anything to make you happy. I want to see your beautiful smile every moment of every day. You bring me joy and happiness, and I promise to give you all of mine. You mean the world to me, and I will forever love you.\n",
            "I love you and have never been so sure of anything in my life. I could get lost in your eyes forever. Whenever I look into them they make me melt, I smile and laugh every time you kiss my neck. Your lips fit with mine perfectly like two pieces of a puzzle coming together.\n",
            "So I guess what I’m trying to say is that I love you. So much. You mean the world to me and I wouldn’t trade you for anything. You are my heart, my soul, and in a million years there will never be anyone else for me but you.\n",
            "It only grew stronger by the day. But as always I didn’t listen to the world, the people around me. If only we could have been brave enough and stuck together. So this is what it means to be without you all these years…\n",
            "You look so fine, your skin so soft. I love seeing you every day. I get butterflies in my stomach anytime we’re this close. I’ve liked you for a while — you’re the one I want to be with forever and ever.\n",
            "You are my everything. I love you more than anything in the world and am so lucky to have you by my side. You mean the world to me and I hope to spend every day making you happy!\n",
            "I think it’s going pretty well. I don’t know of any couples that don’t run into their share of issues along the way, but as far as I can tell, we do ok.\n",
            "Love is a word that shouldn’t be introduced when I am around you because it’s just not needed. It is so apparent that we are meant to be, it’s almost obnoxiously obvious.\n",
            "Every single day, I fall even more in love with you. Your smile melts my heart and your eyes send so many different emotions through me. I can’t imagine life without you.\n",
            "We’ve only been together four months and I already know you are the one. You bring so much happiness into my life, I can’t imagine being without you! I love you!\n",
            "I’d like to share with you what my life was like before I met you. It was boring, old, and meaningless. But now it’s spectacular, new, and meaningful. I LOVE YOU!\n",
            "I love you so much that I would even go as far as lip glossing one of your bras! And someday, if you’re lucky, maybe I’ll even wear it.\n",
            "I hope time and erase everything that you’re feeling bad about. I love to be around you because you are so beautiful! trust me, I am telling the truth.\n",
            "I love you. I’m not saying it because you want me to, or because I have to, or because it’s Valentines day, but because it’s true.\n",
            "Let’s make our relationship status official in case there are any holdouts. We’re dating an\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Take 80% sample twice and perform intersection between them and  check number of lines common.\n",
        "\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"RDDIntersection\")\n",
        "sc = SparkContext.getOrCreate()\n",
        "text_file_path = \"/content/sample_data/lvl.txt\"\n",
        "text_rdd = sc.textFile(text_file_path)\n",
        "sample1 = text_rdd.sample(False, 0.8)\n",
        "sample2 = text_rdd.sample(False, 0.8)\n",
        "common_lines = sample1.intersection(sample2)\n",
        "common_lines_count = common_lines.count()\n",
        "print(\"Number of common lines:\", common_lines_count)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWkUd9vAZmDs",
        "outputId": "bbb3adb3-d3ac-450e-f294-4f0d9ad71b3b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common lines: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the number of records whose income > 8000\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "# Perform the operation: Find the number of records with income > 8000\n",
        "income_gt_8000_count = cars_df.filter(cars_df[\"income\"] > 8000).count()\n",
        "print(\"Number of records with income > 8000:\", income_gt_8000_count)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sqakrTqZzBs",
        "outputId": "27c592e9-3829-44b3-d27d-31be98887346"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records with income > 8000: 295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the number of records whose income <4000 and sales>1000\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Perform the operation: Find the number of records with income < 4000 and sales > 1000\n",
        "filtered_count = cars_df.filter((cars_df[\"income\"] < 4000) & (cars_df[\"sales\"] > 1000)).count()\n",
        "print(\"Number of records with income < 4000 and sales > 1000:\", filtered_count)\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDDJgwkUaCPO",
        "outputId": "5fedf200-f29a-4f6d-de51-d68963b8d103"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records with income < 4000 and sales > 1000: 235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the number of records for different ages and order them in  descending order\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "age_counts = cars_df.groupBy(\"age\").count().orderBy(col(\"count\").desc())\n",
        "age_counts.show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4J0Ko7IaF_m",
        "outputId": "18defd7b-69f0-4a2c-a1e9-38f8098aec51"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 25|   41|\n",
            "| 30|   34|\n",
            "| 27|   33|\n",
            "| 37|   32|\n",
            "| 24|   32|\n",
            "| 28|   30|\n",
            "| 36|   30|\n",
            "| 26|   29|\n",
            "| 29|   29|\n",
            "| 21|   29|\n",
            "| 22|   28|\n",
            "| 20|   28|\n",
            "| 23|   25|\n",
            "| 40|   24|\n",
            "| 49|   24|\n",
            "| 45|   24|\n",
            "| 59|   23|\n",
            "| 46|   23|\n",
            "| 48|   23|\n",
            "| 58|   23|\n",
            "+---+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find mean sales of male(1) and female (0)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "mean_sales_by_gender = cars_df.groupBy(\"gender\").agg(avg(\"sales\").alias(\"mean_sales\"))\n",
        "mean_sales_by_gender.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9wW1gqHaJFH",
        "outputId": "9e4c0a5d-5c81-4585-a20c-006df1483cd8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "|gender|        mean_sales|\n",
            "+------+------------------+\n",
            "|     1|11371.726720647774|\n",
            "|     0|12024.953091684434|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the descriptive statistics of debt\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "debt_stats = cars_df.select(\"debt\").summary()\n",
        "debt_stats.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oeXABrZaN-V",
        "outputId": "e3145cbd-b8bc-48dd-8f69-fcc26bb878d4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|              debt|\n",
            "+-------+------------------+\n",
            "|  count|               963|\n",
            "|   mean|14109.004153686397|\n",
            "| stddev| 18273.70248104874|\n",
            "|    min|                 0|\n",
            "|    25%|              1472|\n",
            "|    50%|              6236|\n",
            "|    75%|             16771|\n",
            "|    max|             59770|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find correlation between miles and debt\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import corr\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "correlation = cars_df.stat.corr(\"miles\", \"debt\")\n",
        "print(\"Correlation between miles and debt:\", correlation)\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG9CQYJxaTUl",
        "outputId": "6235bf23-d618-4795-f8dc-544a1d69e5c8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between miles and debt: 0.5447908747188632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the dataframe to table and write an SQL query for finding  grouping of age,gender and number of records of each group\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import corr\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "correlation = cars_df.stat.corr(\"miles\", \"debt\")\n",
        "print(\"Correlation between miles and debt:\", correlation)\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKmaFxHzadRE",
        "outputId": "66b334f4-529e-4f53-b0ce-d9e671fc65f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between miles and debt: 0.5447908747188632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the dataframe to table and write an SQL query for finding grouping of age,gender and number of records of each group\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "cars_df.createOrReplaceTempView(\"cars_table\")\n",
        "sql_query = \"\"\"\n",
        "SELECT age, gender, COUNT(*) AS record_count\n",
        "FROM cars_table\n",
        "GROUP BY age, gender\n",
        "\"\"\"\n",
        "grouped_records = spark.sql(sql_query)\n",
        "grouped_records.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zBuhUGFah6V",
        "outputId": "cb1de4a6-ddb4-4901-f4bf-5c695a4092c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------------+\n",
            "|age|gender|record_count|\n",
            "+---+------+------------+\n",
            "| 30|     0|          14|\n",
            "| 58|     1|          12|\n",
            "| 48|     1|           9|\n",
            "| 54|     0|           3|\n",
            "| 56|     0|           7|\n",
            "| 25|     1|          20|\n",
            "| 42|     0|           4|\n",
            "| 41|     0|           7|\n",
            "| 29|     0|          13|\n",
            "| 36|     1|          10|\n",
            "| 59|     0|          10|\n",
            "| 23|     0|          10|\n",
            "| 41|     1|           5|\n",
            "| 26|     0|          16|\n",
            "| 39|     0|           6|\n",
            "| 28|     1|          16|\n",
            "| 26|     1|          13|\n",
            "| 31|     1|           7|\n",
            "| 59|     1|          13|\n",
            "| 46|     0|           9|\n",
            "+---+------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the results of question in a csv called carf.csv\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CarsDataFrame\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/cars - a.csv\"\n",
        "cars_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "cars_df.createOrReplaceTempView(\"cars_table\")\n",
        "sql_query = \"\"\"\n",
        "SELECT age, gender, COUNT(*) AS record_count\n",
        "FROM cars_table\n",
        "GROUP BY age, gender\n",
        "\"\"\"\n",
        "grouped_records = spark.sql(sql_query)\n",
        "output_csv_path = \"/content/saple_data/carf.csv\"\n",
        "grouped_records.write.csv(output_csv_path, header=True)\n"
      ],
      "metadata": {
        "id": "ZCLKPYObasoT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preprocessing\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AirQualityPreprocessing\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/airqualitydataset.csv\"\n",
        "airquality_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "valid_column_name = \"PM2_5\"  # Using underscore as a valid alternative\n",
        "renamed_df = airquality_df.withColumnRenamed(\"PM2.5\", valid_column_name)\n",
        "print(\"Initial Schema:\")\n",
        "airquality_df.printSchema()\n",
        "print(\"Sample Data:\")\n",
        "airquality_df.show(5)\n",
        "print(\"Renamed Schema:\")\n",
        "renamed_df.printSchema()\n",
        "print(\"Sample Data with Renamed Column:\")\n",
        "renamed_df.show(5)\n",
        "\n",
        "# Data Preprocessing\n",
        "# Drop rows with missing values\n",
        "preprocessed_df = renamed_df.dropna()\n",
        "selected_columns = [\n",
        "    valid_column_name, \"PM10\", \"NO\", \"NO2\", \"NOx\", \"NH3\",\n",
        "    \"CO\", \"SO2\", \"O3\", \"Benzene\", \"Toluene\", \"Xylene\", \"AQI\"\n",
        "]\n",
        "preprocessed_df = preprocessed_df.select(*selected_columns)\n",
        "print(\"Preprocessed Schema:\")\n",
        "preprocessed_df.printSchema()\n",
        "print(\"Preprocessed Sample Data:\")\n",
        "preprocessed_df.show(5)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXX_RL2fa5sE",
        "outputId": "0c375419-f480-4c11-c77c-dc6983e3ac92"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Schema:\n",
            "root\n",
            " |-- PM2.5: double (nullable = true)\n",
            " |-- PM10: double (nullable = true)\n",
            " |-- NO: double (nullable = true)\n",
            " |-- NO2: double (nullable = true)\n",
            " |-- NOx: double (nullable = true)\n",
            " |-- NH3: double (nullable = true)\n",
            " |-- CO: double (nullable = true)\n",
            " |-- SO2: double (nullable = true)\n",
            " |-- O3: double (nullable = true)\n",
            " |-- Benzene: double (nullable = true)\n",
            " |-- Toluene: double (nullable = true)\n",
            " |-- Xylene: double (nullable = true)\n",
            " |-- AQI: integer (nullable = true)\n",
            "\n",
            "Sample Data:\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "|PM2.5|  PM10|  NO|  NO2|  NOx|  NH3|  CO|  SO2|    O3|Benzene|Toluene|Xylene|AQI|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "| 81.4| 124.5|1.44| 20.5|12.08|10.72|0.12|15.24|127.09|    0.2|    6.5|  0.06|184|\n",
            "|78.32|129.06|1.26| 26.0|14.85|10.28|0.14|26.96|117.44|   0.22|   7.95|  0.08|197|\n",
            "|88.76|135.32| 6.6|30.85|21.77|12.91|0.11|33.59|111.81|   0.29|   7.63|  0.12|198|\n",
            "|64.18|104.09|2.56|28.07|17.01|11.42|0.09| 19.0|138.18|   0.17|   5.02|  0.07|188|\n",
            "|72.47|114.84|5.23| 23.2|16.59|12.25|0.16|10.55|109.74|   0.21|   4.71|  0.08|173|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "Renamed Schema:\n",
            "root\n",
            " |-- PM2_5: double (nullable = true)\n",
            " |-- PM10: double (nullable = true)\n",
            " |-- NO: double (nullable = true)\n",
            " |-- NO2: double (nullable = true)\n",
            " |-- NOx: double (nullable = true)\n",
            " |-- NH3: double (nullable = true)\n",
            " |-- CO: double (nullable = true)\n",
            " |-- SO2: double (nullable = true)\n",
            " |-- O3: double (nullable = true)\n",
            " |-- Benzene: double (nullable = true)\n",
            " |-- Toluene: double (nullable = true)\n",
            " |-- Xylene: double (nullable = true)\n",
            " |-- AQI: integer (nullable = true)\n",
            "\n",
            "Sample Data with Renamed Column:\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "|PM2_5|  PM10|  NO|  NO2|  NOx|  NH3|  CO|  SO2|    O3|Benzene|Toluene|Xylene|AQI|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "| 81.4| 124.5|1.44| 20.5|12.08|10.72|0.12|15.24|127.09|    0.2|    6.5|  0.06|184|\n",
            "|78.32|129.06|1.26| 26.0|14.85|10.28|0.14|26.96|117.44|   0.22|   7.95|  0.08|197|\n",
            "|88.76|135.32| 6.6|30.85|21.77|12.91|0.11|33.59|111.81|   0.29|   7.63|  0.12|198|\n",
            "|64.18|104.09|2.56|28.07|17.01|11.42|0.09| 19.0|138.18|   0.17|   5.02|  0.07|188|\n",
            "|72.47|114.84|5.23| 23.2|16.59|12.25|0.16|10.55|109.74|   0.21|   4.71|  0.08|173|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "Preprocessed Schema:\n",
            "root\n",
            " |-- PM2_5: double (nullable = true)\n",
            " |-- PM10: double (nullable = true)\n",
            " |-- NO: double (nullable = true)\n",
            " |-- NO2: double (nullable = true)\n",
            " |-- NOx: double (nullable = true)\n",
            " |-- NH3: double (nullable = true)\n",
            " |-- CO: double (nullable = true)\n",
            " |-- SO2: double (nullable = true)\n",
            " |-- O3: double (nullable = true)\n",
            " |-- Benzene: double (nullable = true)\n",
            " |-- Toluene: double (nullable = true)\n",
            " |-- Xylene: double (nullable = true)\n",
            " |-- AQI: integer (nullable = true)\n",
            "\n",
            "Preprocessed Sample Data:\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "|PM2_5|  PM10|  NO|  NO2|  NOx|  NH3|  CO|  SO2|    O3|Benzene|Toluene|Xylene|AQI|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "| 81.4| 124.5|1.44| 20.5|12.08|10.72|0.12|15.24|127.09|    0.2|    6.5|  0.06|184|\n",
            "|78.32|129.06|1.26| 26.0|14.85|10.28|0.14|26.96|117.44|   0.22|   7.95|  0.08|197|\n",
            "|88.76|135.32| 6.6|30.85|21.77|12.91|0.11|33.59|111.81|   0.29|   7.63|  0.12|198|\n",
            "|64.18|104.09|2.56|28.07|17.01|11.42|0.09| 19.0|138.18|   0.17|   5.02|  0.07|188|\n",
            "|72.47|114.84|5.23| 23.2|16.59|12.25|0.16|10.55|109.74|   0.21|   4.71|  0.08|173|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter our important columns contributing to AQI\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AirQualityFilterColumns\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/airqualitydataset.csv\"\n",
        "airquality_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "important_columns = [\n",
        "    \"PM2.5\", \"PM10\", \"NO\", \"NO2\", \"NOx\", \"NH3\",\n",
        "    \"CO\", \"SO2\", \"O3\", \"Benzene\", \"Toluene\", \"Xylene\", \"AQI\"\n",
        "]\n",
        "selected_columns = [col(\"`\" + column + \"`\") for column in important_columns]\n",
        "filtered_df = airquality_df.select(*selected_columns)\n",
        "print(\"Filtered Schema:\")\n",
        "filtered_df.printSchema()\n",
        "print(\"Filtered Sample Data:\")\n",
        "filtered_df.show(5)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXZ-x5yNbY3M",
        "outputId": "00c20add-ea28-4f0d-b7a5-6b954b9c9efa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Schema:\n",
            "root\n",
            " |-- PM2.5: double (nullable = true)\n",
            " |-- PM10: double (nullable = true)\n",
            " |-- NO: double (nullable = true)\n",
            " |-- NO2: double (nullable = true)\n",
            " |-- NOx: double (nullable = true)\n",
            " |-- NH3: double (nullable = true)\n",
            " |-- CO: double (nullable = true)\n",
            " |-- SO2: double (nullable = true)\n",
            " |-- O3: double (nullable = true)\n",
            " |-- Benzene: double (nullable = true)\n",
            " |-- Toluene: double (nullable = true)\n",
            " |-- Xylene: double (nullable = true)\n",
            " |-- AQI: integer (nullable = true)\n",
            "\n",
            "Filtered Sample Data:\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "|PM2.5|  PM10|  NO|  NO2|  NOx|  NH3|  CO|  SO2|    O3|Benzene|Toluene|Xylene|AQI|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "| 81.4| 124.5|1.44| 20.5|12.08|10.72|0.12|15.24|127.09|    0.2|    6.5|  0.06|184|\n",
            "|78.32|129.06|1.26| 26.0|14.85|10.28|0.14|26.96|117.44|   0.22|   7.95|  0.08|197|\n",
            "|88.76|135.32| 6.6|30.85|21.77|12.91|0.11|33.59|111.81|   0.29|   7.63|  0.12|198|\n",
            "|64.18|104.09|2.56|28.07|17.01|11.42|0.09| 19.0|138.18|   0.17|   5.02|  0.07|188|\n",
            "|72.47|114.84|5.23| 23.2|16.59|12.25|0.16|10.55|109.74|   0.21|   4.71|  0.08|173|\n",
            "+-----+------+----+-----+-----+-----+----+-----+------+-------+-------+------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into 70, 30 train and test dataframes\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataSplitting\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/airqualitydataset.csv\"\n",
        "airquality_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "train_ratio = 0.7\n",
        "test_ratio = 1.0 - train_ratio\n",
        "train_data, test_data = airquality_df.randomSplit([train_ratio, test_ratio], seed=42)\n",
        "print(\"Number of rows in train_data:\", train_data.count())\n",
        "print(\"Number of rows in test_data:\", test_data.count())\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHWGpO53bdlM",
        "outputId": "32e21829-0862-4546-83af-2b3a67dade57"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in train_data: 7325\n",
            "Number of rows in test_data: 2989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply Linear Regression, Decision Tree classifier and Gradient Boost models on train data frames\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.sql.functions import col\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"ModelTraining\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/airqualitydataset.csv\"\n",
        "airquality_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "feature_columns = [\"PM2.5\", \"PM10\", \"NO\", \"NO2\", \"NOx\", \"NH3\", \"CO\", \"SO2\", \"O3\", \"Benzene\", \"Toluene\", \"Xylene\"]\n",
        "target_column = \"AQI\"\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "feature_vector_df = assembler.transform(airquality_df)\n",
        "train_ratio = 0.7\n",
        "test_ratio = 1.0 - train_ratio\n",
        "train_data, test_data = feature_vector_df.randomSplit([train_ratio, test_ratio], seed=42)\n",
        "linear_regression = LinearRegression(featuresCol=\"features\", labelCol=target_column)\n",
        "linear_model = linear_regression.fit(train_data)\n",
        "decision_tree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=target_column)\n",
        "dt_model = decision_tree.fit(train_data)\n",
        "gradient_boost = GBTClassifier(featuresCol=\"features\", labelCol=target_column)\n",
        "gbt_model = gradient_boost.fit(train_data)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "NUHyJC-DbkZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For each model find predictions on test dataframes and evaluate prediction accuracy.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
        "spark = SparkSession.builder.appName(\"ModelEvaluation\").getOrCreate()\n",
        "csv_file_path = \"/content/sample_data/airqualitydataset.csv\"\n",
        "airquality_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "feature_columns = [\"PM2.5\", \"PM10\", \"NO\", \"NO2\", \"NOx\", \"NH3\", \"CO\", \"SO2\", \"O3\", \"Benzene\", \"Toluene\", \"Xylene\"]\n",
        "target_column = \"AQI\"\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "feature_vector_df = assembler.transform(airquality_df)\n",
        "train_ratio = 0.7\n",
        "test_ratio = 1.0 - train_ratio\n",
        "train_data, test_data = feature_vector_df.randomSplit([train_ratio, test_ratio], seed=42)\n",
        "linear_regression = LinearRegression(featuresCol=\"features\", labelCol=target_column)\n",
        "linear_model = linear_regression.fit(train_data)\n",
        "linear_predictions = linear_model.transform(test_data)\n",
        "decision_tree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=target_column)\n",
        "dt_model = decision_tree.fit(train_data)\n",
        "dt_predictions = dt_model.transform(test_data)\n",
        "gradient_boost = GBTClassifier(featuresCol=\"features\", labelCol=target_column)\n",
        "gbt_model = gradient_boost.fit(train_data)\n",
        "gbt_predictions = gbt_model.transform(test_data)\n",
        "regression_evaluator = RegressionEvaluator(labelCol=target_column)\n",
        "linear_rmse = regression_evaluator.evaluate(linear_predictions, {regression_evaluator.metricName: \"rmse\"})\n",
        "classification_evaluator = MulticlassClassificationEvaluator(labelCol=target_column)\n",
        "dt_accuracy = classification_evaluator.evaluate(dt_predictions, {classification_evaluator.metricName: \"accuracy\"})\n",
        "gbt_accuracy = classification_evaluator.evaluate(gbt_predictions, {classification_evaluator.metricName: \"accuracy\"})\n",
        "print(\"Linear Regression RMSE:\", linear_rmse)\n",
        "print(\"Decision Tree Classifier Accuracy:\", dt_accuracy)\n",
        "print(\"Gradient Boosting Classifier Accuracy:\", gbt_accuracy)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "hSlxZPBCbnfz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}